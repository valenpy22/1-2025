# Data Lake
Repositorio de almacenamiento a gran escala que contiene una gran cantidad de datos sin procesar en su formato nativo hasta que se necesita. Puede manejar datos estructurados, semi-estructurados y no estructurados.

## Capas
### Capa de ingesta de datos
Recopila datos de muchas fuentes diferentes.  
Los datos se pueden recopilar todos a la vez en momentos específicos o se pueden transmitir continuamente en tiempo real.
Apache Kafka puede transmitir datos en tiempo real, Apache Flume puede administrar grandes registros de datos y las herramientas ETL manejan el procesamiento de datos por lotes.

### Capa de almacenamiento
Una vez recopilados los datos, se almacenan como están, sin ningún cambio.
Pueden contener diferentes tipos de datos, desde datos muy organizados (como bases de datos) hasta datos menos organizados (como correos electrónicos o vídeos).

Cuando se controla el consumo en la zona aumentada, empeora el funcionamiento.

Sandbox: Ambiente para jugar, pero los gastos son reducidos y acotados.

Diagramas de tipo C1
### Capa de procesamiento
Utiliza herramientas para clasificar y utilizar los datos según sea necesario.
Hadoop o Apache Spark pueden procesar rápidamente grandes cantidades de datos.

### Capa de análisis y consumo
Aquí se trabajan con los datos, los analizan para encontrar ideas y tomar decisiones.
Utilizan herramientas de análisis para explorar los datos, buscando patrones o información útil que pueda ayudar al negocio. 

## Elefante amarillo
Hadoop (especial para trabajar con grandes volúmenes de daatos). Es una herramienta de EL.
